{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed94750a",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "### (in Numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f2835e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import convolve2d, correlate2d\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00ae4afa",
   "metadata": {},
   "source": [
    "## Getting the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "841374a6",
   "metadata": {},
   "source": [
    "Data shape in general = $x:[n\\times c_{in} \\times h \\times w]$\n",
    "\n",
    "In our case: $n = 16, c_{in} = 1, w = 3, h = 3$\n",
    "\n",
    "For the math derivations, we won't need the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef873b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "C_IN = 1\n",
    "W = 3\n",
    "H = 3\n",
    "C_OUT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4973e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "V_LINE = np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]]).reshape(C_IN, H, W)\n",
    "H_LINE = np.array([[0, 0, 0], [1, 1, 1], [0, 0, 0]]).reshape(C_IN, H, W)\n",
    "CROSS = np.clip(V_LINE + H_LINE, a_min=0, a_max=1).reshape(C_IN, H, W)\n",
    "DIAMOND = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]]).reshape(C_IN, H, W)\n",
    "\n",
    "_, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4) # 1 row, 4 columns\n",
    "\n",
    "\n",
    "ax1.imshow(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]]))\n",
    "ax2.imshow(np.array([[0, 0, 0], [1, 1, 1], [0, 0, 0]]))\n",
    "ax3.imshow(np.clip(np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]]) + np.array([[0, 0, 0], [1, 1, 1], [0, 0, 0]]), a_min=0, a_max=1))\n",
    "ax4.imshow(np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]]))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "SHAPES = {\n",
    "    \"v_line\":V_LINE,\n",
    "    \"h_line\": H_LINE,\n",
    "    \"cross\": CROSS,\n",
    "    \"diamond\": DIAMOND\n",
    "}\n",
    "TARGETS = {\n",
    "    \"v_line\":0,\n",
    "    \"h_line\": 1,\n",
    "    \"cross\": 2,\n",
    "    \"diamond\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c33851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_data_point(shape: str):\n",
    "    return TARGETS[shape], (SHAPES[shape] + np.random.uniform(-0.25, 0.25, size=(C_IN, H, W)))\n",
    "\n",
    "def generate_dataset(n=500, batch_size=16):\n",
    "    def split_into_batches(x, batch_size):\n",
    "        n_batches = len(x) / batch_size\n",
    "        x = np.array_split(x, n_batches)\n",
    "        return np.array(x, dtype=object)\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "\n",
    "    shapes = list(SHAPES.keys())\n",
    "    dataset = [generate_single_data_point(np.random.choice(shapes)) for _ in range(n)]\n",
    "    labels = []\n",
    "    data = []\n",
    "    for y, x in dataset:\n",
    "        labels.append(y)\n",
    "        data.append(x)\n",
    "    \n",
    "    targets = np.array(labels)\n",
    "    data = np.array(data)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.33, random_state=RANDOM_SEED)\n",
    "\n",
    "    X_train = split_into_batches(\n",
    "        X_train, batch_size\n",
    "    )\n",
    "\n",
    "    X_test = split_into_batches(\n",
    "        X_test, batch_size\n",
    "    )\n",
    "    \n",
    "    # Turn the targets into Numpy arrays and flatten the array\n",
    "    y_train = np.array(y_train).reshape(-1, 1)\n",
    "    y_test = np.array(y_test).reshape(-1, 1)\n",
    "\n",
    "    # One-Hot encode the training data and split it into batches (same as with the training data)\n",
    "    one_hot_encoder.fit(y_train)\n",
    "    y_train = one_hot_encoder.transform(y_train).toarray()\n",
    "    y_train = split_into_batches(np.array(y_train), batch_size)\n",
    "\n",
    "    one_hot_encoder.fit(y_test)\n",
    "    y_test = one_hot_encoder.transform(y_test).toarray()\n",
    "    y_test = split_into_batches(np.array(y_test), batch_size)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6aa85733",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "137b6d20",
   "metadata": {},
   "source": [
    "$Y^j_n = B^j + \\sum_{i=0}^{C_{in}}X^i_n \\star_{valid} K^j_i \\quad\\quad \\forall n:[0, \\dots, N]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "710ee298",
   "metadata": {},
   "source": [
    "### Initiating random kernels and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dc530f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403f9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_forward(x: np.array, kernels: np.array, bias: np.array):\n",
    "    c_out, c_in, k, _ = kernels.shape\n",
    "    N, c_in, h_in, w_in = x.shape\n",
    "    h_out = h_in - k + 1\n",
    "    w_out = w_in - k + 1\n",
    "    output_shape = N, c_out, h_out, w_out\n",
    "    output = np.zeros(shape=output_shape)\n",
    "    for n in range(N):\n",
    "        for j in range(c_out):\n",
    "            output[n, j] = bias[j]\n",
    "            for i in range(c_in):\n",
    "                output[n, j] += correlate2d(\n",
    "                    x[n, i], kernels[j, i], mode=\"valid\"\n",
    "                )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cabed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_kernel_gradient(dE_dY: np.array, x: np.array):\n",
    "    n, c_out, h_out, w_out = dE_dY.shape\n",
    "    _, c_in, h_in, w_in = x.shape\n",
    "    k = h_in + 1 - h_out\n",
    "    dK = np.zeros(shape=(c_out, c_in, k, k))\n",
    "    for batch in range(n):\n",
    "        for i in range(c_out):\n",
    "            for j in range(c_in):\n",
    "                dK[i, j] += correlate2d(dE_dY[batch, i], x[batch, j], mode=\"valid\")\n",
    "\n",
    "    return dK / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb11d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_bias_gradient(dE_dY: np.ndarray):\n",
    "    n, c_out, _, _ = dE_dY.shape\n",
    "    dB = dE_dY.sum(axis=(0, 2, 3)) / n\n",
    "    return dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef07c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_input_gradient(dE_dY: np.ndarray, kernels: np.ndarray):\n",
    "    n, c_out, h_out, w_out = dE_dY.shape\n",
    "    c_out, c_in, k, _ = kernels.shape\n",
    "    h_in = h_out + k - 1\n",
    "    w_in = w_out + k - 1\n",
    "    dX = np.zeros((n, c_in, h_in, w_in))\n",
    "\n",
    "    for batch in range(n):\n",
    "        for j in range(c_in):\n",
    "            for i in range(c_out):\n",
    "                dX[batch, j] += convolve2d(dE_dY[batch, i], kernels[i, j], mode=\"full\")\n",
    "    return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eb673ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random_kernels(c_in, c_out, kernel_size):\n",
    "    return np.random.randn(c_out, c_in, kernel_size, kernel_size)\n",
    "\n",
    "\n",
    "def init_random_bias(c_out):\n",
    "    return np.random.randn(c_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d0beb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    def __init__(self, c_in, c_out, kernel_size) -> None:\n",
    "        self.k = init_random_kernels(c_in, c_out, kernel_size)\n",
    "        self.b = init_random_bias(c_out)\n",
    "    \n",
    "    def forward(self, x: np.ndarray):\n",
    "        self.x = x\n",
    "        return conv2d_forward(x, self.k, self.b)\n",
    "\n",
    "    def backward(self, dE_dY: np.ndarray):\n",
    "        dK = conv2d_kernel_gradient(dE_dY, self.x)\n",
    "        dB = conv2d_bias_gradient(dE_dY)\n",
    "        dX = conv2d_input_gradient(dE_dY, self.k)\n",
    "\n",
    "        return {\"dK\": dK, \"dB\": dB, \"dX\": dX}\n",
    "\n",
    "    def update(self, grads, learning_rate):\n",
    "        self.k -= grads[\"dK\"] * learning_rate\n",
    "        self.b -= grads[\"dB\"] * learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5692f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fda01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x: np.ndarray):\n",
    "        self.x = x\n",
    "        n = x.shape[0]\n",
    "        return self.x.reshape((n, -1))\n",
    "\n",
    "    def backward(self, dE_dY: np.ndarray):\n",
    "        return {\"dX\": dE_dY.reshape(self.x.shape)}\n",
    "\n",
    "    def update(self, grads, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f410920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dense_layer(n_in, n_out) -> tuple[np.ndarray, np.ndarray]:\n",
    "    w = np.random.randn(n_in, n_out) * 0.1\n",
    "    b = (\n",
    "        np.random.randn(\n",
    "            n_out,\n",
    "        )\n",
    "        * 0.1\n",
    "    )\n",
    "\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def forward_single_dense_layer(x: np.ndarray, w: np.ndarray, b: np.ndarray):\n",
    "    return x @ w + b\n",
    "\n",
    "\n",
    "def get_weight_gradient_single_dense_layer(x: np.ndarray, dE_dY: np.ndarray):\n",
    "    return x.T @ dE_dY\n",
    "\n",
    "\n",
    "def get_bias_gradient_single_dense_layer(dE_dY: np.ndarray):\n",
    "    return np.sum(dE_dY, axis=0) / dE_dY.shape[0]\n",
    "\n",
    "\n",
    "def get_input_gradient_single_dense_layer(dE_dY: np.ndarray, w: np.ndarray):\n",
    "    return dE_dY @ w.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de0b99e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self) -> None:\n",
    "        self.x = None\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_backward(x):\n",
    "        x[x <= 0] = 0\n",
    "        x[x > 0] = 1\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return self.relu(x)\n",
    "\n",
    "    def backward(self, dE_dY):\n",
    "        dX = np.multiply(dE_dY, self.relu_backward(self.x))\n",
    "        return {\"dX\": dX}\n",
    "\n",
    "    def update(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_backward(x):\n",
    "        sig = Sigmoid.sigmoid(x)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "    def backward(self, dE_dY):\n",
    "        dX = np.multiply(dE_dY, self.sigmoid_backward(self.x))\n",
    "        return {\"dX\": dX}\n",
    "\n",
    "    def update(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, n_in, n_out) -> None:\n",
    "        self.w, self.b = init_dense_layer(n_in, n_out)\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x: np.ndarray):\n",
    "        # print(x.shape)\n",
    "        self.x = x\n",
    "        return forward_single_dense_layer(self.x, self.w, self.b)\n",
    "\n",
    "    def backward(self, dE_dY: np.ndarray):\n",
    "        dW = get_weight_gradient_single_dense_layer(self.x, dE_dY)\n",
    "        dB = get_bias_gradient_single_dense_layer(dE_dY)\n",
    "        dX = get_input_gradient_single_dense_layer(dE_dY, self.w)\n",
    "        return {\"dW\": dW, \"dB\": dB, \"dX\": dX}\n",
    "\n",
    "    def update(self, grad, learning_rate):\n",
    "        self.w -= learning_rate * grad[\"dW\"]\n",
    "        self.b -= learning_rate * grad[\"dB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09ceae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, dE_dY):\n",
    "        grads = []\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(dE_dY)\n",
    "            grads.append(grad)\n",
    "            dE_dY = grad[\"dX\"]\n",
    "\n",
    "        return reversed(grads)\n",
    "\n",
    "    def update(self, learning_rate, grads):\n",
    "        for layer, grad in zip(self.layers, grads):\n",
    "            layer.update(grad, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beee1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(prediction, target):\n",
    "    return 2 * (prediction - target) / np.size(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce3f9654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_accuracy(network, X_test, y_test):\n",
    "    correct = 0\n",
    "    total_counter = 0\n",
    "    for x, y in zip(X_test, y_test):\n",
    "        x = np.array(x, dtype=float)\n",
    "        a = network.forward(x)\n",
    "        pred = np.argmax(a, axis=1, keepdims=True)\n",
    "        y = np.argmax(y, axis=1, keepdims=True)\n",
    "        correct += (pred == y).sum()\n",
    "        total_counter += len(x)\n",
    "    accuracy = correct / total_counter\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8109344",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = generate_dataset(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe85745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Accuracy = 44.24%\n",
      "Epoch 1 Accuracy = 37.58%\n",
      "Epoch 2 Accuracy = 37.58%\n",
      "Epoch 3 Accuracy = 43.64%\n",
      "Epoch 4 Accuracy = 44.85%\n",
      "Epoch 5 Accuracy = 47.27%\n",
      "Epoch 6 Accuracy = 51.52%\n",
      "Epoch 7 Accuracy = 58.79%\n",
      "Epoch 8 Accuracy = 62.42%\n",
      "Epoch 9 Accuracy = 65.45%\n",
      "Epoch 10 Accuracy = 64.85%\n",
      "Epoch 11 Accuracy = 64.24%\n",
      "Epoch 12 Accuracy = 64.24%\n",
      "Epoch 13 Accuracy = 64.24%\n",
      "Epoch 14 Accuracy = 64.24%\n",
      "Epoch 15 Accuracy = 64.24%\n",
      "Epoch 16 Accuracy = 64.24%\n",
      "Epoch 17 Accuracy = 64.24%\n",
      "Epoch 18 Accuracy = 64.24%\n",
      "Epoch 19 Accuracy = 64.24%\n",
      "Epoch 20 Accuracy = 64.24%\n",
      "Epoch 21 Accuracy = 64.24%\n",
      "Epoch 22 Accuracy = 64.24%\n",
      "Epoch 23 Accuracy = 66.06%\n",
      "Epoch 24 Accuracy = 67.88%\n",
      "Epoch 25 Accuracy = 69.09%\n",
      "Epoch 26 Accuracy = 72.12%\n",
      "Epoch 27 Accuracy = 75.15%\n",
      "Epoch 28 Accuracy = 76.97%\n",
      "Epoch 29 Accuracy = 79.39%\n",
      "Epoch 30 Accuracy = 82.42%\n",
      "Epoch 31 Accuracy = 86.06%\n",
      "Epoch 32 Accuracy = 88.48%\n",
      "Epoch 33 Accuracy = 90.3%\n",
      "Epoch 34 Accuracy = 93.94%\n",
      "Epoch 35 Accuracy = 95.76%\n",
      "Epoch 36 Accuracy = 96.97%\n",
      "Epoch 37 Accuracy = 97.58%\n",
      "Epoch 38 Accuracy = 98.18%\n",
      "Epoch 39 Accuracy = 98.79%\n",
      "Epoch 40 Accuracy = 98.79%\n",
      "Epoch 41 Accuracy = 99.39%\n",
      "Epoch 42 Accuracy = 99.39%\n",
      "Epoch 43 Accuracy = 100.0%\n",
      "Epoch 44 Accuracy = 100.0%\n",
      "Epoch 45 Accuracy = 100.0%\n",
      "Epoch 46 Accuracy = 100.0%\n",
      "Epoch 47 Accuracy = 100.0%\n",
      "Epoch 48 Accuracy = 100.0%\n",
      "Epoch 49 Accuracy = 100.0%\n"
     ]
    }
   ],
   "source": [
    "network = Network(\n",
    "    [\n",
    "        Conv2D(1, 4, 2),\n",
    "        Flatten(),\n",
    "        Dense(16, 8),\n",
    "        ReLU(),\n",
    "        Dense(8, 4),\n",
    "        Sigmoid(),\n",
    "    ]\n",
    ")\n",
    "n_epochs = 50\n",
    "learning_rate = 0.1\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        a = network.forward(x)\n",
    "        error = mse_loss(a, y)\n",
    "        grads = network.backward(error)\n",
    "        network.update(learning_rate, grads)\n",
    "    accuracy = get_current_accuracy(network, X_test, y_test)\n",
    "    print(f\"Epoch {epoch} Accuracy = {np.round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13c68864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Accuracy = 91.18%\n",
      "Epoch 1 Accuracy = 92.74%\n",
      "Epoch 2 Accuracy = 93.84%\n",
      "Epoch 3 Accuracy = 94.58%\n",
      "Epoch 4 Accuracy = 94.97%\n",
      "Epoch 5 Accuracy = 95.1%\n",
      "Epoch 6 Accuracy = 95.41%\n",
      "Epoch 7 Accuracy = 95.58%\n",
      "Epoch 8 Accuracy = 95.68%\n",
      "Epoch 9 Accuracy = 95.9%\n",
      "Epoch 10 Accuracy = 96.0%\n",
      "Epoch 11 Accuracy = 96.08%\n",
      "Epoch 12 Accuracy = 96.16%\n",
      "Epoch 13 Accuracy = 96.2%\n",
      "Epoch 14 Accuracy = 96.3%\n",
      "Epoch 15 Accuracy = 96.36%\n",
      "Epoch 16 Accuracy = 96.4%\n",
      "Epoch 17 Accuracy = 96.43%\n",
      "Epoch 18 Accuracy = 96.41%\n",
      "Epoch 19 Accuracy = 96.37%\n",
      "Epoch 20 Accuracy = 96.41%\n",
      "Epoch 21 Accuracy = 96.42%\n",
      "Epoch 22 Accuracy = 96.45%\n",
      "Epoch 23 Accuracy = 96.51%\n",
      "Epoch 24 Accuracy = 96.5%\n",
      "Epoch 25 Accuracy = 96.55%\n",
      "Epoch 26 Accuracy = 96.53%\n",
      "Epoch 27 Accuracy = 96.57%\n",
      "Epoch 28 Accuracy = 96.54%\n",
      "Epoch 29 Accuracy = 96.61%\n",
      "Epoch 30 Accuracy = 96.65%\n",
      "Epoch 31 Accuracy = 96.63%\n",
      "Epoch 32 Accuracy = 96.66%\n",
      "Epoch 33 Accuracy = 96.71%\n",
      "Epoch 34 Accuracy = 96.72%\n",
      "Epoch 35 Accuracy = 96.73%\n",
      "Epoch 36 Accuracy = 96.75%\n",
      "Epoch 37 Accuracy = 96.77%\n",
      "Epoch 38 Accuracy = 96.79%\n",
      "Epoch 39 Accuracy = 96.74%\n",
      "Epoch 40 Accuracy = 96.75%\n",
      "Epoch 41 Accuracy = 96.75%\n",
      "Epoch 42 Accuracy = 96.74%\n",
      "Epoch 43 Accuracy = 96.74%\n",
      "Epoch 44 Accuracy = 96.76%\n",
      "Epoch 45 Accuracy = 96.75%\n",
      "Epoch 46 Accuracy = 96.78%\n",
      "Epoch 47 Accuracy = 96.79%\n",
      "Epoch 48 Accuracy = 96.77%\n",
      "Epoch 49 Accuracy = 96.8%\n"
     ]
    }
   ],
   "source": [
    "from datahandler import get_mnist\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_mnist(batch_size=16, reshaped=True)\n",
    "\n",
    "network = Network(\n",
    "    [\n",
    "        Conv2D(1, 4, 5),\n",
    "        Flatten(),\n",
    "        Dense(2304, 256),\n",
    "        ReLU(),\n",
    "        Dense(256, 128),\n",
    "        ReLU(),\n",
    "        Dense(128, 32),\n",
    "        ReLU(),\n",
    "        Dense(32, 10),\n",
    "        Sigmoid(),\n",
    "    ]\n",
    ")\n",
    "n_epochs = 50\n",
    "learning_rate = 0.1\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        a = network.forward(x)\n",
    "        error = mse_loss(a, y)\n",
    "        grads = network.backward(error)\n",
    "        network.update(learning_rate, grads)\n",
    "    accuracy = get_current_accuracy(network, X_test, y_test)\n",
    "    print(f\"Epoch {epoch} Accuracy = {np.round(accuracy * 100, 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Dec  7 2022, 10:06:04) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "dea301473321733ab39a3a48c3cbaa8a91eca096024cc81bd81940988cf80051"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
